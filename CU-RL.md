- # value function
- # policy gradient


---

- http://www.algorithmdog.com/series/rl-series
- 一、马尔可夫决策过程(Markov Decision Processes, MDP)
- 组成
  - S 表示状态集 (states)；
  - A 表示动作集 (Action)；
  - P_{s,a}^{s'} 表示状态 s 下采取动作 a 之后转移到 s’ 状态的概率；
  - R_{s,a} 表示状态 s 下采取动作 a 获得的奖励；
  - \gamma 是衰减因子。
- 和一般的马尔科夫过程不同，马尔科夫决策过程考虑了动作，即系统下个状态不仅和当前的状态有关，也和当前采取的动作有关
- 还是举下棋的例子，当我们在某个局面（状态s）走了一步 (动作 a )。这时对手的选择（导致下个状态 s’ ）我们是不能确定的，但是他的选择只和 s 和 a 有关，而不用考虑更早之前的状态和动作，即 s’ 是根据 s 和 a 随机生成的
- 分类依据
  - 知道马尔科夫决策过程所有信息(状态集合，动作集合，转移概率和奖励)
  - 知道部分信息 (状态集合和动作集合)
  - 还有些时候马尔科夫决策过程的信息太大无法全部存储 (比如围棋的状态集合总数为3^{19\times 19})。
- 强化学习算法按照上述不同情况可以分为两种
  - 基于模型 (Model-based)  ： 知道并可以存储所有马尔科夫决策过程信息
  - 非基于模型 (Model-free) ： 需要自己探索未知的马尔科夫过程
- 二、Demo
  -   下图是一个机器人从任意一个状态出发寻找金币的例子。找到金币则获得奖励 1，碰到海盗则损失 1。找到金币或者碰到海盗则机器人停止。
    
  - 问题建模成马尔科夫决策过程：
    - 图中不同位置为状态，因此 S = {1,…,8}
    - 机器人采取动作是向东南西北四个方向走，因此A={‘n’,’e’,’s’,’w’}
    - 转移概率方面：
      - 当机器人碰到墙壁，则会停在原来的位置
      - 当机器人找到金币时获得奖励 1，当碰到海盗则损失 1, 其他情况不奖励也不惩罚
    - 除了 R{}_{1,s} = -1 , R_{2,s}=1,R_{5,s}=-1 外，其他情况R_{*,*}=0
- 三、策略和价值
  -  强化学习技术是要学习一个策略 (Policy)，即一个函数：输入为当前状态s，输出为采用动作a的概率\pi(s,a)
  - 最佳策略
    	E_{\pi}[{\sum^{\infty}_{k=0} } \gamma^{k}R_{k}] = E_{\pi}[R_{0}+{\gamma}R_{1}+...]
    - 其中的$$










