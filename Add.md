- 一个完整的已知模型的马尔科夫决策过程可以利用元组来表示。其中 为状态集，为动作集， 为转移概率，也就是对应着环境和智能体的模型，为回报函数，为折扣因子用来计算累积回报
- 累积回报公式为，其中，为有限值时，强化学习过程称为有限范围强化学习，当 时，称为无穷范围强化学习
- 以有限范围强化学习为例进行讲解
- 强化学习的目标是找到最优策略使得累积回报的期望最大
- 所谓策略是指状态到动作的映射 ，用表示从状态 到最终状态的一个序列，则累积回报 是个随机变量
- 随机变量无法进行优化，无法当成是目标函数，采用随机变量的期望作为目标函数，即作为目标函数
- 用公式来表示强化学习的目标为：
- 分类
- 序列决策问题中MDP问题的分类
  	 
- 尔科夫决策过程可以利用元组来描述，根据转移概率是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法
- 两种类别都包括策略迭代算法，值迭代算法和策略搜索算法。不同的是无模型的强化学习方法每类算法又分为online和offline两种
- 基于模型的RL可用DP的思想来解决
- DP的使用条件：
  - 整个优化问题可以分解为多个子优化问题
  - 子优化问题的解可以被存储和重复利用
- MDP利用贝尔曼最优性原理得到贝尔曼最优方程
  - Bellman最优化原理是多阶段决策过程导出泛函方程所依赖的原理，即：一个最优策略有这样的特性，不论初始状态和初始决策如何，相对于第一个决策所形成的状态来说，余下的决策必须构成一个最优策略。
  - 应用Bellman最优化原理必须满足如下两个条件：①目标函数的可分性；②状态的可分性。其中目标函数的可分性是指，对于所有的K，一个多阶段过程的最后K个阶段的目标函数，仅仅取决于当前状态和最后K个决策。状态的可分性是指在一个决策之后，下一个状态仅仅取决于当前的状态与决策，而与前面的状态无关。
