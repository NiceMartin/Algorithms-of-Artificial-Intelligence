tf.placeholder在声明的时候不需要初始化的数值，只需要声明类型和维数，例如
x = tf.placeholder(tf.float32, shape=(None, 1024))
tf.placeholder是为了方便定义神经网络结构，所以可以看作是符号变量。tf.placeholder通常是在训练session开始后，存放输入样本的。

tf.Variable在声明的时候必须要有初始的数值。例如
weights = tf.Variable(tf.random_normal([784, 200], stddev=0.35),name="weights")
biases = tf.Variable(tf.zeros([200]), name="biases")
tf.Variable通常是存放weight和bias，然后会不停地被更新，所以说是variable。


tf.Variable适合一些需要初始化或被训练而变化的权重或参数，而tf.placeholder适合通常不会改变的被训练的数据集。

=============================================================================================
 tf.get_variable 和tf.variable_scope

变量共享主要涉及到两个函数：

tf.get_variable(<name>, <shape>, <initializer>) 和 tf.variable_scope(<scope_name>)。

先来看第一个函数： tf.get_variable。

tf.get_variable 和tf.Variable不同的一点是，前者拥有一个变量检查机制，会检测已经存在的变量是否设置为共享变量，如果已经存在的变量没有设置为共享变量，TensorFlow 运行到第二个拥有相同名字的变量的时候，就会报错。

为了解决这个问题，TensorFlow 又提出了 tf.variable_scope 函数：它的主要作用是，在一个作用域 scope 内共享一些变量，可以有如下几种用法：

1）

with tf.variable_scope("image_filters") as scope:
    result1 = my_image_filter(image1)
    scope.reuse_variables() # or 
    #tf.get_variable_scope().reuse_variables()
    result2 = my_image_filter(image2)
    
需要注意的是：最好不要设置 reuse 标识为 False，只在需要的时候设置 reuse 标识为 True。

2)

with tf.variable_scope("image_filters1") as scope1:
    result1 = my_image_filter(image1)
with tf.variable_scope(scope1, reuse = True)
    result2 = my_image_filter(image2)
    
=============================================================
tf.variable_scope可以让变量有相同的命名，包括tf.get_variable得到的变量，还有tf.Variable的变量
tf.name_scope可以让变量有相同的命名，只是限于tf.Variable的变量

import tensorflow as tf;    
import numpy as np;    
import matplotlib.pyplot as plt;    
  
with tf.variable_scope('V1'):  
    a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
with tf.variable_scope('V2'):  
    a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
    
with tf.Session() as sess:  
    sess.run(tf.initialize_all_variables())  
    print a1.name  
    print a2.name  
    print a3.name  
    print a4.name  

输出：
V1/a1:0
V1/a2:0
V2/a1:0
V2/a2:0

例子2：
import tensorflow as tf;    
import numpy as np;    
import matplotlib.pyplot as plt;    
  
with tf.name_scope('V1'):  
    a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
with tf.name_scope('V2'):  
    a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
    
with tf.Session() as sess:  
    sess.run(tf.initialize_all_variables())  
    print a1.name  
    print a2.name  
    print a3.name  
    print a4.name  
    
报错：Variable a1 already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:

import tensorflow as tf;    
import numpy as np;    
import matplotlib.pyplot as plt;    
  
with tf.name_scope('V1'):  
    # a1 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a2 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
with tf.name_scope('V2'):  
    # a3 = tf.get_variable(name='a1', shape=[1], initializer=tf.constant_initializer(1))  
    a4 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a2')  
    
with tf.Session() as sess:  
    sess.run(tf.initialize_all_variables())  
    # print a1.name  
    print a2.name  
    # print a3.name  
    print a4.name  
   
 输出：
V1/a2:0
V2/a2:0

换成下面的代码就可以执行：
