# Attention
### Reference
+ https://blog.csdn.net/mpk_no1/article/details/72862348
+ https://blog.csdn.net/aliceyangxi1987/article/details/76284315
	+ 如何自动生成文本摘要
		+ https://blog.csdn.net/aliceyangxi1987/article/details/72765285
	+ 使聊天机器人的对话更有营养
		+ https://blog.csdn.net/aliceyangxi1987/article/details/76128058
	+ AM 第一次应用到 NLP
		+ NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE
			+ https://arxiv.org/pdf/1409.0473.pdf
	+ Blog
		+ http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

### Transformer
+ https://www.sohu.com/a/168933829_465914

### 强制前向AM
### 加性注意力（additive attention）
### 乘法（点积）注意力（multiplicative attention）
### 关键值注意力（key-value attention）

### Hierachical Attention Networks for Document Classification
    The idea was proposed in the article by Z. Yang et al., "Hierarchical Attention Networks for Document Classification", 2016: http://www.aclweb.org/anthology/N16-1174.
  - http://www.cnblogs.com/robert-dlut/p/5952032.html

    - Attention机制最早是在视觉图像领域提出来的，应该是在九几年思想就提出来了，但是真正火起来应该算是google mind团队的这篇论文《Recurrent Models of Visual Attention》[14]，他们在RNN模型上使用了attention机制来进行图像分类。随后，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》 [1]中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是是第一个提出attention机制应用到NLP领域中。接着类似的基于attention机制的RNN模型扩展开始应用到各种NLP任务中。最近，如何在CNN中使用attention机制也成为了大家的研究热点。下图表示了attention研究进展的大概趋势。


### ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs


### attention is all your need
	+ http://nlp.seas.harvard.edu/2018/04/03/attention.html
	+ https://github.com/harvardnlp/annotated-transformer

### Hierarchical Attention Networks for Document Classification
- Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.
