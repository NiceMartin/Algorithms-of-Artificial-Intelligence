# GAN
+ 17种GAN变体的Keras实现
	+ https://blog.csdn.net/JohinieLi/article/details/79595146 
+ 干货｜生成式对抗网络GAN的研究进展与展望
	+ http://www.sohu.com/a/167048525_642762
	
	
+ Ian Goodfellow推荐的10篇GAN进展跟踪文献
	+ https://weibo.com/ttarticle/p/show?id=2309404212119326295632
	+ 1. Progressive GANs: Progressive Growing of GANs for Improved Quality, Stability, and Variation
	+ 2. Spectral normalization: Spectral Normalization for Generative Adversarial Networks (got GANs working on lots of classes, which has been hard)
	+ 3. Projection discriminator: cGANs with Projection Discriminator (from the same lab as #2, both techniques work well together, overall give very good results with 1000 classes)
	+ 4. pix2pixHD High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (GANs for 2-megapixel video)​
	+ 5. Are GANs created equal? Are GANs Created Equal? A Large-Scale Study  A big empirical study showing the importance of good rigorous empirical work and how a lot of the GAN variants don't seem to actually offer improvements in practice​
	+ 6. WGAN-GP Improved Training of Wasserstein GANs  : probably the most popular GAN variant today and seems to be pretty good in my opinion. Caveat: the baseline GAN variants should not perform nearly as badly as this paper claims, especially the text one​
	+ 7. StackGAN++: StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks  High quality text-to-image synthesis with GANs​
	+ 8. Making all ML algorithms differentially private by training them on fake private data generated by GANs​ Privacy-preserving generative deep neural networks support clinical data sharing
	+ 9. You should be a little bit aware of the "GANs with encoders" space, one of my favorites is Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks​
	+ 10. You should be a little bit aware of the "theory of GAN convergence" space, one of my favorites is Gradient descent GAN optimization is locally stable​​​​
