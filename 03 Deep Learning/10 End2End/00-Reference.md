# View of End2End

### Attention
- Attention is all you need
- Hierarchical Attention Networks for Document Classification
	- Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.
- 哈佛NLP组论文解读:基于隐变量的注意力模型|附开源代码
	- https://mp.weixin.qq.com/s?src=11&timestamp=1532395473&ver=1017&signature=SIw50DX0hLTIGs8WVbETdcACdb5uxm8CWR*WJ-cAsx6t*GyZ1VTMT4*KsWMoluOwBhaRHT0PS52WRvCd7faepC9ZY3Zf7UTOJGeqqdIBu7lAK2mDu4T08ihYrDr8lucf&new=1
