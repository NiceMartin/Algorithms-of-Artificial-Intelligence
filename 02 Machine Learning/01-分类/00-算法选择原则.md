# ML Algorithm

## Reference
+ https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
+ https://www.zhihu.com/question/26726794/answer/421409624
+ 机器学习算法集锦：从贝叶斯到深度学习及各自优缺
	+ https://zhuanlan.zhihu.com/p/25327755

## Repersentation, Evaluation, Optimization
+ 对于supervised learning的机器学习算法而言，机器学习算法可以拆解为representation、evaluation和optimization三个部分。
+ 具体的，假设 x 是训练集中一条sample的data， y 为该条sample的label， $ y^{\hat{}}=f(x;w) $为预测值，那么：
	+ <1> f(x;w) 的具体形式就是representation，比如是一次型的 $ f(x;w)=w^Tx+b $，或者二次型的 $ f(x;w)=x^Twx+b $
	+ <2> 衡量 y 和 $ y^{\hat{}} $ 之间差距的是evaluation，其实也就是loss function，例如我们熟知的squared loss， 
	$$ loss(y, y^\hat{}) = \frac{1}2||y^\hat{}-y||^2= \frac{1}2||f(x;w)-y||^2 $$
        
       ![](https://pic2.zhimg.com/80/v2-dbce9350c5050e1154f0b7c54ff7fd31_hd.jpg)
       
       + <3>根据<2>中的evaluation求解权重 w 的过程则是optimization，包括我们熟知的SGD、EM等都可以划到optimization。

## 为一个具体场景选择算法
+ 搞清楚这个场景的数据分布
	+ 找到representation和该分布契合的模型，例如该场景的数据分布是一次型的，那我们就可以选择logistic regression、SVM等分界面为一次型的模型；如果场景的数据分布是二次型的，我们可以选择naive bayes；如果场景的数据分布既不是一次型也不是二次型，那我们可以选择基于决策树的模型，例如gbdt、random forest等，或者DNN，这些模型都高度非线性，表达能力极强，理论上可以拟合任意曲线
+ 该模型的optimization过程硬件能否承受
	+ 如果场景数据分布是一次型，但是训练集数据量极大，那我们一般会选logistic regression，而放弃SVM，因为SVM的optimization过程对大数据量不太友好。
+ 具体来说
	+ 具体地，logistic regression和SVM（linear kernel）的representation都是一次型的，它们不同的地方在于evaluation和optimization，如果数据分布是一次型的，用这两个差别不会太大，但是logistic regression的optimization过程对大数据量更加友好，而且预测值能有概率意义，所以工业界使用logistic regression更多
	+ 另外一个工业界用得非常多的模型是gbdt，它的representation类似于下图。其实基于决策树的模型都是通过一个个平行于坐标轴的平面去拟合训练集的实际分界面，理论上平行于坐标轴的平面能够拟合任意分界面，这一点类似于DNN。实际场景中，数据分界面为非线性的情况占大多数，gbdt一方面继承了决策树的强表达能力，另外一方面又规避决策树variance太大的问题。
	+ adaboost属于ensemble method中boosting方法的一个具体实现，ensemble method包括bagging、boosting和stacking。这些方法在打比赛的时候常用，因为理论上它们一定会带来效果上的提升。
