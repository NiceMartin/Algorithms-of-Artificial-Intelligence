# Ensemble Learning

## Rerference
+ [1] http://www.daicoolb.ml/xgboost-vs-lightbgm/
+ [2] GBDT
	+ https://blog.csdn.net/w28971023/article/details/8240756

## 如何集成？

## 概率知识
+ Bootstraping: 名字来自成语“pull up by your own bootstraps”，意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下：
　　（1） 采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。 
　　（2） 根据抽出的样本计算给定的统计量T。 
　　（3） 重复上述N次（一般大于1000），得到N个统计量T。 
　　（4） 计算上述N个统计量T的样本方差，得到统计量的方差。
　　应该说Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。

+ Jackknife： 和上面要介绍的Bootstrap功能类似，只是有一点细节不一样，即每次从样本中抽样时候只是去除几个样本（而不是抽样），就像小刀一样割去一部分。

## Bagging
+ 解释
	+ 简单model 有较大bias 较小得variance
	+ 复杂model 较小bias 较大variance
	+ 将多个复杂得model 求平均值，bias 得平均值基本不变，variance 可能接近正确值
	+ 其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。
+ 训练：有放回抽样N份数据，将数据经过一个复杂得分类器，将N个结果投票或求平均
+ 测试：测试数据经过N个分类器，将N个结果投票(分类)或求平均（回归）
+ 使用前提：一个 model 很复杂，可能存在overfitting, 例如 Decision Tree(NN 相对不容易Overfitting)

+ A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
+ B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树. 感知器等）
+ C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

## Boosting
+ 使用前提 ： 使用在一系列弱的分类器上
+ 核心问题：
	+ 1）在每一轮如何改变训练数据的权值或概率分布？
		通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
	+ 2）通过什么方式来组合弱分类器？
		通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

## Bagging 和 Boosting 得区别
+ bagging 是一个较复杂模型在一系列抽样数据上获得结果，boosting 是一个一些列教简单得模型在一份会变化得样本上获得结果
+ 1）样本选择上：
	+ Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
	+ Boosting：使用全部数据，不抽样，每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
+ 2）样例权重：
	+ Bagging：使用均匀取样，每个样例的权重相等
	+ Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
+ 3）预测函数：
	+ Bagging：所有预测函数的权重相等。
	+ Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
+ 4）并行计算：
	+ Bagging：各个预测函数可以并行生成
	+ Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

## Stacking
+ 训练数据分为两部分 A 和 B
+ 数据A训练N个不同得分类器C1
+ 数据A经过N个训练好的分类器C1后，得到N个输出，但这N个输出也有好有坏
+ 可以使用数据B再训练一个较简单得分类器C2(C1认为已经训练好，C1参数不在变化)，确定C2的参数

## 组合方法
+ 1）Bagging + 决策树 = 随机森林  （很多 深度较大得决策树）
+ 2）AdaBoost + 决策树 = 提升树	(很多 深度较小的决策树)
+ 3）Gradient Boosting + 决策树 = GBDT(Gradient Boosting Decision Tree)


## Random Forest
+ 随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行. 列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。
+ 一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。

+ 原理:
	+ 提到随机森林，就不得不提Bagging，Bagging可以简单的理解为：放回抽样，多数表决（分类）或简单平均（回归）,同时Bagging的基学习器之间属于并列生成，不存在强依赖关系。
	+ Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括RF包括四个部分：
		1. 随机选择样本（放回抽样）；
		2. 随机**选择特征**；
		3. 构建决策树；
		4. 随机森林投票（平均）。
	+ 随机选择样本和Bagging相同，随机选择特征是指在树的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属 性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。
	+ RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。
	+ **RF和Bagging对比**：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。
+ 优缺点
	+ 随机森林的优点较多，简单总结：
	1. 在数据集上表现良好，相对于其他算法有较大的优势（训练速度. 预测准确度）；
	2. 能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；
	3. 容易做成并行化方法。
+ RF的缺点：在噪声较大的分类或者回归问题上回过拟合。

## Random forest与bagging的区别：
+ 1）Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；
+ 2）bagging是用全部特征来得到分类器，而random forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Random forest效果比bagging效果好！

## Decision Tree 与 Random Forest 的区别
+ DT 在全部属性 中 找到一个最优属性来构建树的当前节点
+ RF 在全部属性中先随机找K个属性，再在K个随机属性中找一个来构建书的当前节点

## Adaboost
+ PAC 定义了学习算法的强弱
	- 弱学习算法---识别错误率小于1/2(即准确率仅比随机猜测略高的学习算法)
	- 强学习算法---识别准确率很高并能在多项式时间内完成的学习算法
+ 权重的调整，参见LHY PPT

+ https://blog.csdn.net/haidao2009/article/details/7514787

## Gradient Boosting
+ 更一般的boosting

## GBDT
+ GBDT主要由三个概念组成：Regression Decistion Tree（即DT)，Gradient Boosting（即GB)，Shrinkage (算法的一个重要演进分枝，目前大部分源码都按该版本实现）
	+ 回归树


+ 提GBDT之前，谈一下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。
- 由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。
+ 原理
	- GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。
	- 在GradientBoosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。
	- GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。
+ 优缺点
	- GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显， 1. 它能灵活的处理各种类型的数据； 2. 在相对较少的调参时间下，预测的准确度较高。
	- **当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据**。






## XGBoost的四大改进:
+ 改进残差函数 不用Gini作为残差，用二阶泰勒展开+树的复杂度（正则项） 带来如下好处：
	1. 可以控制树的复杂度
	2. 带有关于梯度的更多信息，获得了二阶导数
	3. 可以用线性分类器
+ 采用预排序 因为每一次迭代中，都要生成一个决策树，而这个决策树是残差的决策树，所以传统的不能并行 但是陈天奇注意到，每次建立决策树，在分裂节点的时候，比如选中A特征，就要对A进行排序，再计算残差，这个花很多时间 于是陈天奇想到，每一次残差计算好之后，全部维度预先排序，并且此排序是可以并行的，并行排序好后，对每一个维度，计算一次最佳分裂点，求出对应的残差增益 于是只要不断选择最好的残差作为分裂点就可以。 也就是说，虽然森林的建立是串行的没有变，但是每一颗树枝的建立就变成是并行的了，带来的好处：
	1. 分裂点的计算可并行了，不需要等到一个特征的算完再下一个了
	2. 每层可以并行： 当分裂点的计算可以并行，对每一层，比如分裂了左儿子和右儿子，那么这两个儿子上分裂哪个特征及其增益也计算好了
+ Shrinkage（缩减） 相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）
+ 列抽样 XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算。

## LightGBM，我只谈3点优化：
+ 采用基于Histogram的决策树算法 把每个特征做转化成int，并用这个int作为直方图的index，如果某一个特征值的值为ki，就在直方图横轴=ki的地方，增加1的高度 最后根据直方图进行分裂 带来的好处：
	1. 不用计算分裂增益
	2. 只消耗很少的内存，解决xgboost为了排序需要把特征都加进内存需要巨大的空间
+ 带深度限制的Leaf-wise的叶子生长策略。 直接找到分裂增益最大的叶子，按层优先不断分裂
	1. 提高精度降低误差
	2. 减少Level-wise非常非常的无用叶子的分裂
	3. 因为特征的访问顺序相同，就可以提高cache优化，意味着CPU可以为下一次会采用的特征预先做预读取
+ 用histogram 做差加速 一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到 也就是说下一次分裂的时候不需要计算分裂增益，直接计算一个大儿子，另一个小儿子的直方图就是父亲减去大儿子的差 1.进一步优化


## 集成半监督学习的改进

