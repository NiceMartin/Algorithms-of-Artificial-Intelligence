# Ensemble Learning

## Rerference
+ [1] http://www.daicoolb.ml/xgboost-vs-lightbgm/
+ [2] GBDT
	+ https://blog.csdn.net/w28971023/article/details/8240756

## 如何集成？

## 概率知识
+ Bootstraping: 名字来自成语“pull up by your own bootstraps”，意思是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下：
	+ 采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。 
	+ 根据抽出的样本计算给定的统计量T。 
	+ 重复上述N次（一般大于1000），得到N个统计量T。 
	+ 计算上述N个统计量T的样本方差，得到统计量的方差。
+ 应该说Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。
+ Jackknife： 和上面要介绍的Bootstrap功能类似，只是有一点细节不一样，即每次从样本中抽样时候只是去除几个样本（而不是抽样），就像小刀一样割去一部分。

## Bagging
+ 解释
	+ 简单model 有较大bias 较小得variance
	+ 复杂model 较小bias 较大variance
	+ 将多个复杂得model 求平均值，bias 得平均值基本不变，variance 可能接近正确值
	+ 其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。
+ 训练：有放回抽样N份数据，将数据经过一个复杂得分类器，将N个结果投票或求平均
+ 测试：测试数据经过N个分类器，将N个结果投票(分类)或求平均（回归）
+ 使用前提：一个 model 很复杂，可能存在overfitting, 例如 Decision Tree(NN 相对不容易Overfitting)

+ A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
+ B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树. 感知器等）
+ C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

## Boosting
+ 使用前提 ： 使用在一系列弱的分类器上
+ 核心问题：
	+ 1）在每一轮如何改变训练数据的权值或概率分布？
		通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
	+ 2）通过什么方式来组合弱分类器？
		通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

## Bagging 和 Boosting 得区别
+ bagging 是一个较复杂模型在一系列抽样数据上获得结果，boosting 是一个一些列教简单得模型在一份会变化得样本上获得结果
+ 1）样本选择上：
	+ Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
	+ Boosting：使用全部数据，不抽样，每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
+ 2）样例权重：
	+ Bagging：使用均匀取样，每个样例的权重相等
	+ Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
+ 3）预测函数：
	+ Bagging：所有预测函数的权重相等。
	+ Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
+ 4）并行计算：
	+ Bagging：各个预测函数可以并行生成
	+ Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
+ 不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。
- 由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。

## Stacking
+ 训练数据分为两部分 A 和 B
+ 数据A训练N个不同得分类器C1
+ 数据A经过N个训练好的分类器C1后，得到N个输出，但这N个输出也有好有坏
+ 可以使用数据B再训练一个较简单得分类器C2(C1认为已经训练好，C1参数不在变化)，确定C2的参数

## 组合方法
+ 1）Bagging + 决策树 = 随机森林  （很多 深度较大得决策树）
+ 2）AdaBoost + 决策树 = 提升树	(很多 深度较小的决策树)
+ 3）Gradient Boosting + 决策树 = GBDT(Gradient Boosting Decision Tree)

## 集成半监督学习的改进

